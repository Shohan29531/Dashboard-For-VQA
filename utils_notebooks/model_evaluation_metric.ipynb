{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d5309ab1",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0de1da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/imrankabir/Desktop/research/vqa_accessibility/Dashboard-For-VQA/Dashboard Data\"\n",
    "gt_fol = os.path.join(data_path, \"GT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fa2b4106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_common_obj = ['Person', 'Bicycle', 'Car', 'Motorcycle', 'Bus', 'Traffic Signals', 'Fire hydrant', 'Stop sign'\n",
    "                   'Bench', 'Dog', 'Chair', 'Vegetation']\n",
    "\n",
    "pfb_common_obj = ['Road', 'Sidewalk', 'Tree', 'Vegetation', 'Building', 'Fence', 'Traffic Signals',\n",
    "                  'Fire hydrant', 'Chair', 'Trash on roads', 'Trash bins', 'Person', 'Car', 'Motorcycle',\n",
    "                  'Bus']\n",
    "\n",
    "com_obj = list(set(coco_common_obj) & set(pfb_common_obj))\n",
    "\n",
    "len(com_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "04e78bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mAP(targets, preds):\n",
    "    APs = np.zeros(preds.shape[1])\n",
    "    APs2 = np.zeros(preds.shape[1])\n",
    "    ARs = np.zeros(preds.shape[1])\n",
    "    F1s = np.zeros(preds.shape[1])\n",
    "    ACCs = np.zeros(preds.shape[1])\n",
    "    for k in range(preds.shape[1]):\n",
    "        APs[k], ARs[k], F1s[k], ACCs[k] = _average_precision_2(targets[:, k], preds[:, k])\n",
    "        APs2[k] = _average_precision(targets[:, k], preds[:, k])\n",
    "\n",
    "    return APs, ARs, F1s, ACCs, APs2\n",
    "\n",
    "def _average_precision(output, target):\n",
    "    # print(output, target)\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # sort examples\n",
    "    indices = output.argsort()[::-1]\n",
    "    # Computes prec@i\n",
    "    total_count_ = np.cumsum(np.ones((len(output), 1)))\n",
    "\n",
    "    target_ = target[indices]\n",
    "\n",
    "    ind = target_ == 1\n",
    "\n",
    "    pos_count_ = np.cumsum(ind)\n",
    "\n",
    "    total = pos_count_[-1]\n",
    "    pos_count_[np.logical_not(ind)] = 0\n",
    "\n",
    "    pp = pos_count_ / total_count_\n",
    "    precision_at_i_ = np.sum(pp)\n",
    "    # print(pp, precision_at_i_, pos_count_, total_count_)\n",
    "    precision_at_i = precision_at_i_ / (total + epsilon)\n",
    "\n",
    "    return precision_at_i\n",
    "\n",
    "\n",
    "def _average_precision_2(target, output):\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    total_pred_pos = np.count_nonzero(output)\n",
    "    total_target_pos = np.count_nonzero(target)\n",
    "\n",
    "    TP = np.count_nonzero(np.logical_and(output, target))\n",
    "    TN = np.count_nonzero(np.logical_and(np.logical_not(output), np.logical_not(target)))\n",
    "    FP = total_pred_pos - TP\n",
    "    FN = total_target_pos - TP\n",
    "\n",
    "    AP = TP / (TP + FP + epsilon)\n",
    "    AR = TP / (TP + FN + epsilon)\n",
    "    F1 = (2 * AP * AR) / (AP + AR + epsilon)\n",
    "    ACC = (TP + FP) / (TP + TN + FP + FN + epsilon)\n",
    "\n",
    "    return AP, AR, F1, ACC\n",
    "\n",
    "\n",
    "def calculate_model_map(gt_path, pred_path, obj_list=None):\n",
    "    gt_dict = {}\n",
    "    pred_dict = {}\n",
    "    \n",
    "    gt_files = [x for x in os.listdir(gt_path) if x.endswith('.csv') and int(x.split('-')[1])<=16]\n",
    "\n",
    "    for fl in gt_files:\n",
    "        if fl in [\"video-10-segment-1.csv\", \"video-9-segment-2.csv\"]:\n",
    "            continue\n",
    "        gt_fl = os.path.join(gt_path, fl)\n",
    "        pred_fl = os.path.join(pred_path, fl)\n",
    "        if not os.path.exists(pred_fl):\n",
    "            continue\n",
    "\n",
    "        gt_data = pd.read_csv(gt_fl)\n",
    "        gt_data.replace(-1, 1, inplace=True) \n",
    "        x = []\n",
    "        for index, row in gt_data.iterrows():\n",
    "            if list(row)[0].strip() in [\"Sidewalk pits\"]:\n",
    "                continue\n",
    "            if list(row)[0].strip() in gt_dict.keys():\n",
    "                gt_dict[list(row)[0].strip()] += list(row)[1:]\n",
    "            else:\n",
    "                gt_dict[list(row)[0].strip()] = list(row)[1:]\n",
    "\n",
    "            x.append(len(list(row)[1:]))\n",
    "\n",
    "        y = []    \n",
    "        pred_data = pd.read_csv(pred_fl)\n",
    "        pred_data.replace(-1, 1, inplace=True) \n",
    "        for index, row in pred_data.iterrows():\n",
    "            if list(row)[0].strip() in [\"Sidewalk pits\"]:\n",
    "                continue\n",
    "            if list(row)[0].strip() in pred_dict.keys():\n",
    "                pred_dict[list(row)[0].strip()] += list(row)[1:]\n",
    "            else:\n",
    "                pred_dict[list(row)[0].strip()] = list(row)[1:]\n",
    "            y.append(len(list(row)[1:]))\n",
    "            \n",
    "        if x != y:\n",
    "            print(len(x), len(y), fl)\n",
    "    \n",
    "    target_array = []\n",
    "    pred_array = []\n",
    "    \n",
    "    label_names = []\n",
    "\n",
    "    for key in gt_dict.keys():\n",
    "        if key in pred_dict.keys():\n",
    "            if obj_list:\n",
    "                if key not in obj_list:\n",
    "                    continue\n",
    "            if len(gt_dict[key]) == len(pred_dict[key]):\n",
    "                target_array.append(gt_dict[key])\n",
    "                pred_array.append(pred_dict[key])\n",
    "                label_names.append(key)\n",
    "            else:\n",
    "                print(len(gt_dict[key]), len(pred_dict[key]), key)\n",
    "\n",
    "    target_array = np.array(target_array).T\n",
    "    pred_array = np.array(pred_array).T\n",
    "    \n",
    "    try:\n",
    "        ap_2 = np.array(metrics.precision_score(target_array, pred_array, average=None))\n",
    "        ap, ar, f1, acc, ap_3 = get_mAP(target_array, pred_array)\n",
    "        report = classification_report(target_array, pred_array,target_names=label_names)\n",
    "    except Exception as e:\n",
    "        # ap = 0\n",
    "        print(e)\n",
    "        ap, ar, f1, acc, report = [0], [0], [0], [0], []\n",
    "        print(target_array.shape, pred_array.shape)\n",
    "    \n",
    "    return ap, ar, f1, acc, report, ap_2, ap_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "814fb635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "      macro avg       0.46      0.60      0.48      1053\n",
      "   weighted avg       0.89      0.85      0.86      1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_fol = os.path.join(data_path, \"BLIP\")\n",
    "apg, arg, f1g, accg, rep, apg2, apg3 = calculate_model_map(\n",
    "    gt_fol, pred_fol, \n",
    "    obj_list=coco_common_obj\n",
    ")\n",
    "\n",
    "rep = rep.split('\\n')[0:1] + rep.split('\\n')[-4:-2]\n",
    "rep = \"\\n\".join(rep)\n",
    "print(rep)\n",
    "# print(apg.mean(), apg2.mean(), apg3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "03a6b3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "      macro avg       0.51      0.55      0.44      1053\n",
      "   weighted avg       0.89      0.82      0.83      1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_fol = os.path.join(data_path, \"GPV-1\")\n",
    "apg, arg, f1g, accg, rep, apg2, apg3 = calculate_model_map(\n",
    "    gt_fol, pred_fol, \n",
    "    obj_list=coco_common_obj\n",
    ")\n",
    "\n",
    "rep = rep.split('\\n')[0:1] + rep.split('\\n')[-4:-2]\n",
    "rep = \"\\n\".join(rep)\n",
    "print(rep)\n",
    "# print(apg.mean(), apg2.mean(), apg3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "021e60ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "      macro avg       0.67      0.52      0.54      1053\n",
      "   weighted avg       0.95      0.71      0.72      1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_fol = os.path.join(data_path, \"yolo_v7\")\n",
    "apg, arg, f1g, accg, rep, apg2, apg3 = calculate_model_map(\n",
    "    gt_fol, pred_fol, \n",
    "    obj_list=coco_common_obj\n",
    ")\n",
    "\n",
    "rep = rep.split('\\n')[0:1] + rep.split('\\n')[-4:-2]\n",
    "rep = \"\\n\".join(rep)\n",
    "print(rep)\n",
    "# print(apg.mean(), apg2.mean(), apg3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "bccb8904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "      macro avg       0.30      0.68      0.36      1053\n",
      "   weighted avg       0.73      0.91      0.79      1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_fol = os.path.join(data_path, \"HRNet_V2\")\n",
    "apg, arg, f1g, accg, rep, apg2, apg3 = calculate_model_map(\n",
    "    gt_fol, pred_fol, \n",
    "    obj_list=coco_common_obj\n",
    ")\n",
    "\n",
    "rep = rep.split('\\n')[0:1] + rep.split('\\n')[-4:-2]\n",
    "rep = \"\\n\".join(rep)\n",
    "print(rep)\n",
    "# print(apg.mean(), apg2.mean(), apg3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "5338f227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "      macro avg       0.39      0.69      0.41      1053\n",
      "   weighted avg       0.84      0.82      0.78      1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_fol = os.path.join(data_path, \"mask_rcnn\")\n",
    "apg, arg, f1g, accg, rep, apg2, apg3 = calculate_model_map(\n",
    "    gt_fol, pred_fol, \n",
    "    obj_list=coco_common_obj\n",
    ")\n",
    "\n",
    "rep = rep.split('\\n')[0:1] + rep.split('\\n')[-4:-2]\n",
    "rep = \"\\n\".join(rep)\n",
    "print(rep)\n",
    "# print(apg.mean(), apg2.mean(), apg3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "703a4fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "      macro avg       0.40      0.70      0.42      1053\n",
      "   weighted avg       0.85      0.83      0.79      1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_fol = os.path.join(data_path, \"faster_rcnn\")\n",
    "apg, arg, f1g, accg, rep, apg2, apg3 = calculate_model_map(\n",
    "    gt_fol, pred_fol, \n",
    "    obj_list=coco_common_obj\n",
    ")\n",
    "\n",
    "rep = rep.split('\\n')[0:1] + rep.split('\\n')[-4:-2]\n",
    "rep = \"\\n\".join(rep)\n",
    "print(rep)\n",
    "# print(apg.mean(), apg2.mean(), apg3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3839b1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "      macro avg       0.22      0.43      0.21      1053\n",
      "   weighted avg       0.64      0.49      0.51      1053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/imrankabir/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred_fol = os.path.join(data_path, \"Random\")\n",
    "apg, arg, f1g, accg, rep, apg2, apg3 = calculate_model_map(\n",
    "    gt_fol, pred_fol, \n",
    "    obj_list=coco_common_obj\n",
    ")\n",
    "\n",
    "rep = rep.split('\\n')[0:1] + rep.split('\\n')[-4:-2]\n",
    "rep = \"\\n\".join(rep)\n",
    "print(rep)\n",
    "# print(apg.mean(), apg2.mean(), apg3.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de408e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
