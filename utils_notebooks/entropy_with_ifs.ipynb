{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-01T22:11:08.028765Z",
     "start_time": "2023-11-01T22:11:02.916274Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import lpips\n",
    "import math\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from natsort import natsorted\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import scipy.linalg as linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imrankabir/Desktop/research/vqa_accessibility/Dashboard-For-VQA/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/imrankabir/Desktop/research/vqa_accessibility/Dashboard-For-VQA/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/imrankabir/Desktop/research/vqa_accessibility/Dashboard-For-VQA/venv/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imrankabir/Desktop/research/vqa_accessibility/Dashboard-For-VQA/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/imrankabir/Desktop/research/vqa_accessibility/Dashboard-For-VQA/venv/lib/python3.9/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "loss_fn_alex = lpips.LPIPS(net='alex')\n",
    "loss_fn_vgg = lpips.LPIPS(net='vgg')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T22:11:15.798057Z",
     "start_time": "2023-11-01T22:11:13.938294Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "images_dir = '/Users/imrankabir/Desktop/research/vqa_accessibility/Dashboard-For-VQA/Dashboard Data/Images'\n",
    "data_path = '/Users/imrankabir/Desktop/research/vqa_accessibility/Dashboard-For-VQA/Dashboard Data'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T22:11:15.799308Z",
     "start_time": "2023-11-01T22:11:15.793362Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "transform_f = transforms.ToTensor()\n",
    "\n",
    "def normalize_image(in_img):\n",
    "    pixels = np.asarray(in_img).astype('float32')\n",
    "    pixels = (pixels - mean) / std\n",
    "    return pixels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T22:11:15.799523Z",
     "start_time": "2023-11-01T22:11:15.793628Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_steady_state_probabilities_ifs(pred, img_dir, vid_n, seg_n):\n",
    "    pred = pred.T\n",
    "    unq_st = np.unique(pred, axis=0)\n",
    "    unq_st_str = []\n",
    "    for x in unq_st:\n",
    "        str_bit = [str(ch) for ch in x]\n",
    "        unq_st_str.append(''.join(str_bit))\n",
    "\n",
    "    transition_matrix_dict = {\n",
    "        'st': [x for x in unq_st_str]\n",
    "    }\n",
    "    for u_s_s in unq_st_str:\n",
    "        transition_matrix_dict[u_s_s] = [0.0 for _ in unq_st_str]\n",
    "\n",
    "    transition_matrix = pd.DataFrame(transition_matrix_dict)\n",
    "    transition_matrix = transition_matrix.set_index('st')\n",
    "\n",
    "    for f in range(1, pred.shape[0]):\n",
    "        s_now = ''.join([str(ch) for ch in pred[f]])\n",
    "        s_prev = ''.join([str(ch) for ch in pred[f-1]])\n",
    "        f_now_pth = os.path.join(img_dir, f'video-{vid_n}-segment-{seg_n}-frame-{f}.jpeg')\n",
    "        f_prev_pth = os.path.join(img_dir, f'video-{vid_n}-segment-{seg_n}-frame-{f-1}.jpeg')\n",
    "\n",
    "        image_now = cv2.resize(normalize_image(np.array(Image.open(\n",
    "            f_now_pth\n",
    "        ).convert('RGB'))/255), (64, 64), interpolation = cv2.INTER_LINEAR).astype(np.float32)\n",
    "        image_prev = cv2.resize(normalize_image(np.array(Image.open(\n",
    "            f_prev_pth\n",
    "        ).convert('RGB'))/255), (64, 64), interpolation = cv2.INTER_LINEAR).astype(np.float32)\n",
    "\n",
    "        img0 = transform_f(image_now).unsqueeze(0)\n",
    "        img1 = transform_f(image_prev).unsqueeze(0)\n",
    "\n",
    "        d = loss_fn_alex(img0, img1).detach().numpy()[0,0,0,0]\n",
    "\n",
    "        transition_matrix[s_prev][s_prev] += (1.0*d)\n",
    "        transition_matrix[s_now][s_prev] += (1.0*(1-d))\n",
    "\n",
    "    for ind, row in transition_matrix.iterrows():\n",
    "        row = row/(row.sum()+1e-15)\n",
    "        transition_matrix.loc[ind] = row\n",
    "\n",
    "    transition_matrix = np.array(transition_matrix)\n",
    "\n",
    "    I = np.identity(transition_matrix.shape[0])\n",
    "    P_I = transition_matrix - I\n",
    "    co_eff = P_I.T\n",
    "\n",
    "    co_eff[co_eff.shape[0]-1] =  np.ones((co_eff.shape[1]))\n",
    "    const = np.array([0.0 for _ in range(co_eff.shape[0])])\n",
    "    const[const.shape[0]-1] = 1.0\n",
    "\n",
    "    p_s_ifs = np.linalg.solve(co_eff, const)\n",
    "\n",
    "    return p_s_ifs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T22:11:16.049206Z",
     "start_time": "2023-11-01T22:11:16.046750Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_steady_state_probabilities(h_m):\n",
    "    hm = np.array(h_m).T\n",
    "    unique_states = np.unique(hm, axis=0)\n",
    "    unq_st_and_count = {}\n",
    "    for i, u_s in enumerate(unique_states):\n",
    "        c = np.argwhere(np.all(hm == u_s, axis=-1)).shape[0]\n",
    "        unq_st_and_count[i] = {\n",
    "            'val': u_s,\n",
    "            'count': c,\n",
    "            'ss_prob': c/hm.shape[0]\n",
    "        }\n",
    "\n",
    "    return np.array([unq_st_and_count[k]['ss_prob'] for k in unq_st_and_count.keys()])\n",
    "\n",
    "\n",
    "def calculate_entropy(ss_probs):\n",
    "    if len(ss_probs) <= 1:\n",
    "        return 0.0\n",
    "\n",
    "    tot_ss_ent = 0\n",
    "\n",
    "    for prb in ss_probs:\n",
    "        if prb == 0:\n",
    "            log_p_ss = 0\n",
    "        else:\n",
    "            log_p_ss = math.log2(prb)\n",
    "\n",
    "        t_ent = - prb * log_p_ss\n",
    "\n",
    "        tot_ss_ent = tot_ss_ent + t_ent\n",
    "\n",
    "    tot_ss_ent = tot_ss_ent / math.log2(len(ss_probs))\n",
    "\n",
    "    return tot_ss_ent"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T22:11:16.567474Z",
     "start_time": "2023-11-01T22:11:16.545608Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_st_p_ent(vid, seg, img_path, objs, model):\n",
    "    pred_file = os.path.join(\n",
    "        data_path,\n",
    "        f'{model}/video-{vid}-segment-{seg}.csv'\n",
    "    )\n",
    "    pred_df = pd.read_csv(pred_file)\n",
    "    pred_df = pred_df.transpose()\n",
    "    pred_df.columns = pred_df.iloc[0]\n",
    "    pred_df = pred_df.iloc[1:]\n",
    "    pred_df = pred_df.reindex(columns=objs).fillna('0').transpose()\n",
    "\n",
    "    p_steady_ifs = get_steady_state_probabilities_ifs(np.array(pred_df), images_dir, vid, seg)\n",
    "    ent_ifs = calculate_entropy(p_steady_ifs)\n",
    "\n",
    "    p_steady = get_steady_state_probabilities(np.array(pred_df))\n",
    "    ent = calculate_entropy(p_steady)\n",
    "\n",
    "    return p_steady, ent, p_steady_ifs, ent_ifs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T22:11:17.022311Z",
     "start_time": "2023-11-01T22:11:16.987792Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "v_ = 2\n",
    "s_ = 1\n",
    "\n",
    "objs_ = [\n",
    "    'Car', 'Person', 'Curb', 'Tree', 'Barrier Post'\n",
    "]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T22:11:17.457792Z",
     "start_time": "2023-11-01T22:11:17.448111Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1875     0.02083333 0.02083333 0.04166667 0.0625     0.1875\n",
      " 0.02083333 0.29166667 0.16666667] 0.8344157297354335\n",
      "[0.13042024 0.0263023  0.01614984 0.04423116 0.07952279 0.24763723\n",
      " 0.02174713 0.24342108 0.19056823] 0.844704382064666\n"
     ]
    }
   ],
   "source": [
    "model_ = 'BLIP'\n",
    "p_s, e_, p_s_i, e_i = get_st_p_ent(v_, s_, images_dir, objs_, model_)\n",
    "print(p_s, e_)\n",
    "print(p_s_i, e_i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T22:11:22.511868Z",
     "start_time": "2023-11-01T22:11:17.894395Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08333333 0.04166667 0.02083333 0.0625     0.10416667 0.08333333\n",
      " 0.16666667 0.04166667 0.04166667 0.04166667 0.3125    ] 0.8738809887518927\n",
      "[0.06469828 0.04355108 0.01075841 0.06911155 0.10472037 0.10283319\n",
      " 0.17364232 0.03000038 0.03639326 0.04142975 0.32286142] 0.8523952496012203\n"
     ]
    }
   ],
   "source": [
    "model_ = 'GPV-1'\n",
    "p_s, e_, p_s_i, e_i = get_st_p_ent(v_, s_, images_dir, objs_, model_)\n",
    "print(p_s, e_)\n",
    "print(p_s_i, e_i)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T22:11:29.178236Z",
     "start_time": "2023-11-01T22:11:24.663572Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
